import os
import numpy as np
import joblib
from transformers import AutoTokenizer, AutoModel
import logging
import argparse
import torch
import requests
from dotenv import load_dotenv
import json
from pathlib import Path
from typing import Dict, List, Optional, Union

__all__ = ['predict_vulnerabilities']

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# --- Configuration ---
MODELS_DIR = Path(__file__).parent.parent / "models"
MODEL_FILE = MODELS_DIR / "codebert_vuln_classifier.joblib"
TOKENIZER_NAME = "microsoft/codebert-base"

# Global cache for tokenizer and model
tokenizer_global = None
model_global = None
device_global = None

def load_codebert_model_and_tokenizer():
    global tokenizer_global, model_global, device_global
    if tokenizer_global is None or model_global is None:
        try:
            logging.info(f"Loading CodeBERT tokenizer ({TOKENIZER_NAME})...")
            tokenizer_global = AutoTokenizer.from_pretrained(TOKENIZER_NAME)
            logging.info(f"Loading CodeBERT model ({TOKENIZER_NAME})...")
            model_global = AutoModel.from_pretrained(TOKENIZER_NAME)
            
            device_global = torch.device("cuda" if torch.cuda.is_available() else "cpu")
            model_global.to(device_global)
            logging.info(f"CodeBERT tokenizer and model loaded successfully on {device_global}.")
        except Exception as e:
            logging.error(f"Failed to load CodeBERT model or tokenizer: {e}")
            raise
    return tokenizer_global, model_global, device_global

def get_embedding(code_snippet, tokenizer, model, device):
    try:
        inputs = tokenizer(code_snippet, return_tensors="pt", truncation=True, max_length=512, padding=True)
        inputs = {k: v.to(device) for k, v in inputs.items()}
        with torch.no_grad():
            outputs = model(**inputs)
        embedding = outputs.last_hidden_state.mean(dim=1).cpu().numpy().flatten()
        return embedding
    except Exception as e:
        logging.error(f"Error generating embedding: {e}\nSnippet (first 100 chars): {code_snippet[:100]}")
        return None

def fetch_code_from_etherscan(address, api_key):
    """Fetch contract source code from Etherscan using V2 API.
    
    Args:
        address (str): Ethereum contract address (0x...)
        api_key (str): Etherscan API key
        
    Returns:
        str: Source code if successful, None otherwise
        
    Raises:
        ValueError: If address or API key is invalid
        requests.RequestException: If there's an HTTP error
        json.JSONDecodeError: If the response is not valid JSON
        Exception: For other errors during the fetch
    """
    if not address or not isinstance(address, str) or len(address) != 42 or not address.startswith('0x'):
        raise ValueError("Invalid Ethereum address format")
        
    if not api_key or not isinstance(api_key, str):
        raise ValueError("Invalid Etherscan API key")
    
    logging.info(f"Fetching source code for contract: {address} using Etherscan V2 API")
    
    def process_source_code(source_code, address):
        """Process the raw source code from Etherscan."""
        # Handle JSON-encoded source code
        if source_code.startswith("{") and source_code.endswith("}"):
            try:
                json_input = json.loads(source_code)
                
                # Handle standard JSON input format
                if 'sources' in json_input and isinstance(json_input['sources'], dict):
                    all_sources = ""
                    for file_path, content_obj in json_input['sources'].items():
                        if 'content' in content_obj:
                            all_sources += f"// File: {file_path}\n{content_obj['content']}\n\n"
                    
                    if all_sources:
                        logging.info(f"Processed multi-file contract from {address}")
                        return all_sources.strip()
                    
                    logging.warning(f"Standard JSON input for {address} has no valid content")
                
                # Handle Truffle-style format
                elif 'source' in json_input and isinstance(json_input['source'], dict):
                    all_sources = "".join(
                        f"// File: {path}\n{content['content']}\n\n"
                        for path, content in json_input['source'].items()
                        if 'content' in content
                    )
                    
                    if all_sources:
                        logging.info(f"Processed Truffle-style multi-file contract from {address}")
                        return all_sources.strip()
                    
                    logging.warning(f"Truffle-style input for {address} has no valid content")
                
                # Unknown JSON format, return as-is
                logging.info(f"Unknown JSON format for {address}, returning raw content")
                return source_code
                
            except json.JSONDecodeError:
                # Not valid JSON, treat as plain Solidity
                logging.info(f"Fetched plain Solidity code for {address}")
                return source_code
        
        # Handle non-JSON source code
        logging.info(f"Fetched source code for {address}")
        return source_code
    
    # Common parameters for both V1 and V2 APIs
    params = {
        'module': 'contract',
        'action': 'getsourcecode',
        'address': address,
        'apikey': api_key
    }
    
    def try_fetch_source_code(api_url, is_v2=True):
        """Helper function to try fetching source code from a specific API version"""
        try:
            response = requests.get(api_url, params=params, timeout=30)
            response.raise_for_status()
            data = response.json()
            
            # Check for successful response
            if str(data.get('status')) == '1' and data.get('result') and len(data['result']) > 0:
                source_code_info = data["result"][0]
                if source_code_info.get("SourceCode"):
                    source_code = source_code_info["SourceCode"]
                    logging.info(f"Successfully fetched source code using {'V2' if is_v2 else 'V1'} API")
                    return process_source_code(source_code, address)
            
            return None
            
        except (requests.exceptions.RequestException, json.JSONDecodeError) as e:
            logging.warning(f"Error with {'V2' if is_v2 else 'V1'} API: {e}")
            return None
    
    # Try V2 API first
    v2_source = try_fetch_source_code("https://api.etherscan.io/v2/api", is_v2=True)
    if v2_source:
        return v2_source
    
    # Fall back to V1 API
    v1_source = try_fetch_source_code("https://api.etherscan.io/api", is_v2=False)
    if v1_source:
        return v1_source
    
    logging.error(f"Failed to fetch source code for {address} after trying both V1 and V2 APIs")
    return None


def analyze_code_content(code_content: str, source_name_for_log: str, classifier, tokenizer, model, device) -> Dict[str, Optional[float]]:
    """Analyze code content for vulnerabilities.
    
    Args:
        code_content (str): The source code to analyze
        source_name_for_log (str): Name of the source for logging
        classifier: The trained classifier model
        tokenizer: CodeBERT tokenizer
        model: CodeBERT model
        device: PyTorch device
        
    Returns:
        dict: Analysis results including vulnerability score
    """
    try:
        logging.info(f"Analyzing {source_name_for_log}...")
        
        # Get code embedding
        code_embedding = get_embedding(code_content, tokenizer, model, device)
        
        if code_embedding is None:
            logging.warning(f"Skipping analysis for {source_name_for_log} due to embedding failure.")
            return {
                'vulnerability_score': None,
                'source_name': source_name_for_log,
                'is_vulnerable': False,
                'error': 'Embedding generation failed'
            }
        
        # Make prediction
        prediction = classifier.predict_proba([code_embedding])[0]
        vulnerability_score = prediction[1]  # Probability of being vulnerable
        
        logging.info(f"Analysis complete for {source_name_for_log}. Vulnerability score: {vulnerability_score:.4f}")
        
        return {
            'vulnerability_score': float(vulnerability_score),
            'source_name': source_name_for_log,
            'is_vulnerable': vulnerability_score > 0.5
        }
        
    except Exception as e:
        logging.error(f"Error analyzing {source_name_for_log}: {str(e)}")
        return {
            'vulnerability_score': None,
            'source_name': source_name_for_log,
            'is_vulnerable': False,
            'error': str(e)
        }

def predict_vulnerabilities(code_content: str, source_name: str = 'Unknown') -> Dict[str, Optional[float]]:
    """Predict vulnerabilities in the given code.
    
    Args:
        code_content (str): The source code to analyze
        source_name (str): Name of the source for logging
        
    Returns:
        dict: Analysis results including vulnerability score
    """
    try:
        # Load model and tokenizer
        load_codebert_model_and_tokenizer()
        
        # Load classifier
        if not MODEL_FILE.exists():
            raise FileNotFoundError(f"Classifier model not found at {MODEL_FILE}")
            
        classifier = joblib.load(MODEL_FILE)
        
        # Run analysis
        return analyze_code_content(
            code_content,
            source_name,
            classifier,
            tokenizer_global,
            model_global,
            device_global
        )
        
    except Exception as e:
        logging.error(f"Error in vulnerability prediction: {str(e)}")
        return {
            'error': str(e),
            'vulnerability_score': None,
            'source_name': source_name
        }

def main():
    env_path = Path(__file__).parent.parent / ".env"
    load_dotenv(env_path)
    api_key = os.getenv("ETHERSCAN_API_KEY")

    parser = argparse.ArgumentParser(description="Predict vulnerability of a Solidity contract via address, file, or direct code.")
    group = parser.add_mutually_exclusive_group(required=True)
    group.add_argument("--address", type=str, help="Ethereum contract address to analyze (fetches from Etherscan).")
    group.add_argument("--file", type=str, dest="contract_path", help="Path to the local .sol contract file.")
    group.add_argument("--code", type=str, help="Direct Solidity code string to analyze.")
    
    args = parser.parse_args()

    # Load CodeBERT model and tokenizer
    try:
        tokenizer, model, device = load_codebert_model_and_tokenizer()
    except Exception:
        return # Error already logged

    # Load the trained classifier
    if not MODEL_FILE.exists():
        logging.error(f"Trained model not found: {MODEL_FILE}")
        logging.error("Please ensure 'codebert_vuln_classifier.joblib' exists in the 'models' directory.")
        return
    
    try:
        classifier = joblib.load(MODEL_FILE)
        logging.info(f"Loaded trained classifier from {MODEL_FILE}")
    except Exception as e:
        logging.error(f"Error loading trained classifier: {e}")
        return

    code_to_analyze = None
    source_name_for_log = "Unknown_Source"

    if args.address:
        source_name_for_log = args.address
        if not api_key:
            logging.error("ETHERSCAN_API_KEY not found in .env file. Cannot fetch code for address.")
            logging.error("Please ensure your .env file is in the project root (zauriscore/.env) and contains ETHERSCAN_API_KEY.")
            return
        code_to_analyze = fetch_code_from_etherscan(args.address, api_key)
        if not code_to_analyze:
            # Error already logged by fetch_code_from_etherscan
            return
    elif args.contract_path:
        source_name_for_log = os.path.basename(args.contract_path)
        if not os.path.exists(args.contract_path):
            logging.error(f"Contract file not found: {args.contract_path}")
            return
        try:
            with open(args.contract_path, "r", encoding="utf-8", errors="ignore") as f:
                code_to_analyze = f.read()
            logging.info(f"Successfully read code from file: {args.contract_path}")
        except Exception as e:
            logging.error(f"Failed to read contract file {args.contract_path}: {e}")
            return
    elif args.code:
        source_name_for_log = "direct_code_input"
        code_to_analyze = args.code
        logging.info("Processing direct code input.")


    if not code_to_analyze or not code_to_analyze.strip():
        logging.error(f"No valid code provided or obtained for {source_name_for_log}. Analysis cannot proceed.")
        return

    result = analyze_code_content(code_to_analyze, source_name_for_log, classifier, tokenizer, model, device)
    if result:
        print(f"\nVulnerability Analysis for {source_name_for_log}:")
        print(f"Score: {result['vulnerability_score']:.4f}")
        print(f"Is Vulnerable: {'Yes' if result['is_vulnerable'] else 'No'}")
    else:
        print("Analysis failed.")

if __name__ == "__main__":
    main()